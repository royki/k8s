# Excersice

- How many applications do you see hosted on the cluster? (Check the number of deployments)
  - `kubectl get deployments -o wide`
- On which nodes are the applications hosted on? (Check pods)
  - `kubectl get pods -o wide`
- We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable. Node node01 Unschedulable & Pods evicted from node01

```console
**cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet**
kubectl drain node01 --ignore-daemonsets
```

- The maintenance tasks have been completed. Configure the node to be schedulable again. Node01 is Schedulable.
  - `kubectl uncordon node01`
- Node03 has our critical applications. We do not want to schedule any more apps on node03. Mark node03 as unschedulable but do not remove any apps currently running on it . Node03 Unschedulable & Node03 has apps
  -`kubectl cordon node03`

```console
master $ kubectl get deployments -o wide
NAME   READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR
blue   3/3     3            3           2m31s   nginx        nginx:alpine   app=blue
red    2/2     2            2           2m31s   nginx        nginx:alpine   app=red

master $ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
blue-68df9fb9d9-4745h   1/1     Running   0          3m6s   10.38.0.1   node02   <none>           <none>
blue-68df9fb9d9-965zn   1/1     Running   0          3m6s   10.34.0.2   node03   <none>           <none>blue-68df9fb9d9-thbl7   1/1     Running   0          3m6s   10.40.0.3   node01   <none>           <none>
red-5dc59c9654-p8rcw    1/1     Running   0          3m6s   10.40.0.2   node01   <none>           <none>red-5dc59c9654-vsnv2    1/1     Running   0          3m6s   10.34.0.1   node03   <none>           <none>
```

## In Master Node

- `kubeadm upgrade plan`
- `kubeadm upgrade plan v1.17.0`
- `apt install kubeadm=1.17.0-00`
- `kubeadm upgrade apply v1.17.0`
- `apt install kubelet=1.17.0-00`
- `apt-mark hold kubeadm`
- `apt-mark hold kubelet`

## In Worker Nodes

- `kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2)`
- `apt install kubelet=1.17.0-00`
- `apt install kubeadm=1.17.0-00` (Don't need in Worker Nodes)
- `apt-mark hold kubeadm`
- `apt-mark hold kubelet`

```yaml
 etcd
      --advertise-client-urls=https://172.17.0.35:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --initial-advertise-peer-urls=https://172.17.0.35:2380
      --initial-cluster=master=https://172.17.0.35:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.35:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.17.0.35:2380
      --name=master
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
```

- `kubectl logs etcd-master -n kube-system`
- `kubectl describe pod etcd-master -n kube-system`
- At what address do you reach the ETCD cluster from your master node?
  - `--listen-client-urls=https://127.0.0.1:2379,https://172.17.0.35:2379`
- Where is the ETCD server certificate file located?
  - `--key-file=/etc/kubernetes/pki/etcd/server.key`
- Where is the ETCD CA Certificate file located?
  - `--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt`

## Backup

- Resource Configuration
  - `kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml`
  - Velero from HeptIO
- ETCD Cluster

```console
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot-pre-boot.db
```

- ETCDCTL_API=3 etcdctl snapshot save snapshot.db
- ETCDCTL_API=3 etcdctl snapshot status snapshot.db
- Restore ETCD
  - `service kube-apiserver stop`
  - `ETCDCTL_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup`
  - `systemctl daemon-reload`
  - `service etcd restart`
  - `service kube-apiserver start`
- Persistent Volumes

## Restore

```console
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --name=master \
     --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
     --data-dir /var/lib/etcd-from-backup \
     --initial-cluster=master=https://127.0.0.1:2380 \
     --initial-cluster-token etcd-cluster-1 \
     --initial-advertise-peer-urls=https://127.0.0.1:2380 \
     snapshot restore /tmp/snapshot-pre-boot.db
```

```console
ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/etcd/ca.crt \
    --cert==/etc/etcd/etcd-server.crt \
    --key==/etc/etcd/etcd-server.key \
```

- `export ETCDCTL_API=3`
- `etcdctl version`
- `etcdctl snapshot save -h`
- `etcdctl snapshot restore -h`
- `kubectl describe pod etcd-controlplane -n kube-system` and look for `--listen-client-urls`
- etcd server certificate file location - `--cert-file=/etc/kubernetes/pki/etcd/server.crt`
- etcd ca certificate file location -- `--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt`
