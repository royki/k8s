# Node Upgrade

- If a Node goes down, PODs on that are not accessable. Depending on deployment, PODs of that node are recreated on other nodes.
- If the Node comes back immediately, the `kubelet` process starts and the PODs also come back online.
- If the Node is down for more than 5 mins, the PODs are terminated from that Node. Kubernetes considers those PODs are dead.
  - If the PODs are part of ReplicaSet, then they are recreated on other Nodes.
  - The time, it waits for POD to come back online is known as `Pod Eviction TimeOut`.
  - `Pod Eviction TimeOut` is set by default on the kube controller manager is 5mins.
  - `kube-controller-manager --pod-eviction-timeout=5m0s ...`
  - When a Node goes offline, the Master Node waits for upto 5 mins before considering the Node is dead.
- When a Node comes back online after the pod-eviction-timeout, it doesn't have any scheduled POD, since the existing PODs are rescheduled/recreated by the Scheduler/ReplicaSet on other Nodes.
- So to prevent PODs from going down during a Node maintenance time, we can shift the workload/PODs of the Node to other available Nodes.
  - `kubectl drain Node_Name`
    - `drain` technically destroys the PODs from that Node and recreates those PODs to other Nodes.
    - Aswell the drained Node is marked as cordoned & unschedulable.
  - When the Node comes back online, it is still unschedulable. To mark it as schedulable -
    - `kubectl uncordon Node_Name`
    - The PODs dont't automatically comeback to the `uncordon` Node.
    - When there is new PODs are scheduled, the PODs can be deployed on the `uncordon` Node.
- There is another command - `cordon`
  - `kubectl cordon Node_Name`
  - `cordon` a Node means, it is unschedulable.
  - The PODs on `cordon` Node are not destoryed or moved to other Nodes.
  - It only make sure that new POD will not be deployed on that Node.

- `kubectl drain node01 --ignore-daemonsets`
- `kubectl uncordon node01`
- POD doesn't create on Master Node because - taint is applied on Master Node.
  - `Taints: node-role.kubernetes.io/master:NoSchedule`
- If a POD in a Node is not a part of ReplicaSet, we can't drain that Node as the POD will be lost forever. We need to forcefully drain the POD (cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet)
  - `kubectl drain node02 --ignore-daemonsets --force`
